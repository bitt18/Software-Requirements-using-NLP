{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e7bd1e9",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt_tab')\n",
    "\n",
    "STOPWORDS = set(stopwords.words('english'))\n",
    "df = pd.read_csv('NFR_data.csv', encoding='latin1')\n",
    "\n",
    "REPLACE_BY_SPACE = re.compile('[/(){}\\|@,;]')\n",
    "BAD_SYMBOLS = re.compile('[^0-9a-z #+_]')\n",
    "\n",
    "ignore_words = [\n",
    "    'i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you',\n",
    "    \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself',\n",
    "    'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her',\n",
    "    'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them',\n",
    "    'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this',\n",
    "    'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were',\n",
    "    'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does',\n",
    "    'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because',\n",
    "    'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about',\n",
    "    'against', 'between', 'into', 'through', 'during', 'before', 'after',\n",
    "    'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off',\n",
    "    'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there',\n",
    "    'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few',\n",
    "    'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only',\n",
    "    'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will',\n",
    "    'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm',\n",
    "    'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\",\n",
    "    'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\",\n",
    "    'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn',\n",
    "    \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\",\n",
    "    'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\",\n",
    "    '?', '%', '/', '(', ')', '[', ']', '-', ':', ';', 'system', 'product',\n",
    "    'application', 'should', 'would', 'shall', 'go', 'System', 'system.',\n",
    "    'System.','â€',\"'\",]\n",
    "\n",
    "def clean_nfr(text):\n",
    "    text = text.lower()\n",
    "    text = REPLACE_BY_SPACE.sub(' ', text)\n",
    "    text = BAD_SYMBOLS.sub('', text)\n",
    "    text = ' '.join(word for word in text.split() if word not in STOPWORDS)\n",
    "    text = ' '.join(word for word in text.split() if word not in ignore_words)\n",
    "    return text\n",
    "\n",
    "def tokenize_and_create_vocab(df, text_column):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    vocab = set()\n",
    "    tokenized_texts = []\n",
    "    for text in df[text_column]:\n",
    "        cleaned_text = clean_nfr(text)\n",
    "        tokens = word_tokenize(cleaned_text)\n",
    "        lemmatized_tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "        vocab.update(lemmatized_tokens)\n",
    "        tokenized_texts.append(lemmatized_tokens)\n",
    "    return tokenized_texts, vocab\n",
    "\n",
    "def replace_oov(tokens, vocab):\n",
    "    return [token if token in vocab else 'UKN' for token in tokens]\n",
    "\n",
    "df['cleaned_sentence'] = df['sentence'].apply(clean_nfr)\n",
    "tokenized_texts, vocab = tokenize_and_create_vocab(df, 'cleaned_sentence')\n",
    "df['tokenized'] = tokenized_texts\n",
    "df['tokenized_with_oov'] = df['tokenized'].apply(lambda tokens: replace_oov(tokens, vocab))\n",
    "print(df[\"tokenized_with_oov\"])\n",
    "\n",
    "def word_embedding(df, text_column, max_words=5000, max_length=50):\n",
    "    tokenizer = Tokenizer(num_words=max_words, oov_token='OOV')\n",
    "    tokenizer.fit_on_texts(df[text_column])\n",
    "    sequences = tokenizer.texts_to_sequences(df[text_column])\n",
    "    padded_sequences = pad_sequences(sequences, maxlen=max_length, padding='post', truncating='post')\n",
    "    print(\"Padded Sequences Shape:\", padded_sequences.shape)\n",
    "    return padded_sequences, tokenizer\n",
    "\n",
    "padded_sequences, tokenizer = word_embedding(df, 'cleaned_sentence')\n",
    "np.save('padded_sequences.npy', padded_sequences)\n",
    "\n",
    "print(\"Padded sequences saved to 'padded_sequences.npy'\")\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9350df24",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.callbacks import ReduceLROnPlateau, EarlyStopping\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "tfidv = TfidfVectorizer(max_features=5000, min_df=1, smooth_idf=True, norm='l2')\n",
    "X = tfidv.fit_transform(df['sentence'].values)\n",
    "Y = pd.get_dummies(df['class_name']).values\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(\n",
    "    X, Y, test_size=0.20, random_state=34, stratify=Y\n",
    ")\n",
    "\n",
    "def create_model(input_dim):\n",
    "    model = Sequential([\n",
    "        Dense(2128, activation=\"relu\", input_dim=input_dim),\n",
    "        Dropout(0.3),\n",
    "        Dense(256, activation=\"relu\"),\n",
    "        Dropout(0.5),\n",
    "        Dense(128, activation=\"relu\"),\n",
    "        Dropout(0.3),\n",
    "        Dense(5, activation=\"softmax\")\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=5, min_lr=1e-6, verbose=1)\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=10, verbose=1)\n",
    "\n",
    "model = create_model(X.shape[1])\n",
    "history = model.fit(\n",
    "    X_train, Y_train,\n",
    "    epochs=5, batch_size=32,\n",
    "    validation_data=(X_test, Y_test),\n",
    "    callbacks=[reduce_lr, early_stop], verbose=2\n",
    ")\n",
    "\n",
    "def plot_history(history):\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
    "    plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "    plt.title('Model Accuracy')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history.history['loss'], label='Train Loss')\n",
    "    plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "    plt.title('Model Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "plot_history(history)\n",
    "\n",
    "y_train_pred = model.predict(X_train)\n",
    "y_train_pred_classes = np.argmax(y_train_pred, axis=1)\n",
    "y_train_true = np.argmax(Y_train, axis=1)\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "y_true = np.argmax(Y_test, axis=1)\n",
    "\n",
    "cm = confusion_matrix(y_true, y_pred_classes)\n",
    "ax = sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "ax.set_title('Confusion Matrix')\n",
    "ax.set_xlabel('Predicted')\n",
    "ax.set_ylabel('Actual')\n",
    "plt.show()\n",
    "\n",
    "target_names = df['class_name'].unique()\n",
    "print(\"Classification Report (Training Set):\")\n",
    "print(classification_report(y_train_true, y_train_pred_classes, target_names=target_names))\n",
    "print(classification_report(y_true, y_pred_classes, target_names=target_names))\n",
    "\n",
    "train_acc = model.evaluate(X_train, Y_train, verbose=False)[1]\n",
    "test_acc = model.evaluate(X_test, Y_test, verbose=False)[1]\n",
    "print(f\"Training Accuracy: {train_acc:.4f}\")\n",
    "print(f\"Testing Accuracy: {test_acc:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
