{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b433f4d4",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "!pip install evaluate\n",
    "import pandas as pd\n",
    "import torch\n",
    "from bs4 import BeautifulSoup\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from datasets import Dataset\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, DataCollatorWithPadding, TrainingArguments, Trainer\n",
    "import evaluate \n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dbdc7a8",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "data_path = \"NFR_data.csv\"\n",
    "text_column_name = \"sentence\"\n",
    "label_column_name = \"class_name\"\n",
    "model_name = \"microsoft/deberta-v3-base\"\n",
    "test_size = 0.2\n",
    "\n",
    "class Cleaner():\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def normalize_line_breaks(self, text):\n",
    "        return str(text).replace('\\r\\n', '\\n').replace('\\r', '\\n')\n",
    "\n",
    "    def remove_html_tags(self, text):\n",
    "        return BeautifulSoup(str(text), \"lxml\").text\n",
    "\n",
    "    def clean(self, text):\n",
    "        text = self.normalize_line_breaks(text)\n",
    "        text = self.remove_html_tags(text)\n",
    "        return text\n",
    "\n",
    "try:\n",
    "    df = pd.read_csv(data_path, encoding=\"latin1\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: The file {data_path} was not found. Please ensure it's in the correct path.\")\n",
    "    exit()\n",
    "except Exception as e:\n",
    "    print(f\"Error reading CSV: {e}\")\n",
    "    exit()\n",
    "\n",
    "df.dropna(subset=[text_column_name, label_column_name], inplace=True)\n",
    "df['text_cleaned'] = df[text_column_name].astype(str).apply(Cleaner().clean)\n",
    "\n",
    "le = preprocessing.LabelEncoder()\n",
    "df['label'] = le.fit_transform(df[label_column_name].astype(str).tolist())\n",
    "num_labels_actual = len(le.classes_)\n",
    "print(f\"Found {num_labels_actual} unique labels: {le.classes_}\")\n",
    "\n",
    "df_trainval, df_test = train_test_split(\n",
    "    df,\n",
    "    test_size=0.2,\n",
    "    stratify=df['label'],\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "df_train, df_val = train_test_split(\n",
    "    df_trainval,\n",
    "    test_size=0.125,\n",
    "    stratify=df_trainval['label'],\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "train_dataset = Dataset.from_pandas(df_train)\n",
    "val_dataset = Dataset.from_pandas(df_val)\n",
    "test_dataset = Dataset.from_pandas(df_test)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    return tokenizer(examples[\"text_cleaned\"], truncation=True, padding=True)\n",
    "\n",
    "tokenized_train = train_dataset.map(preprocess_function, batched=True)\n",
    "tokenized_val = val_dataset.map(preprocess_function, batched=True)\n",
    "tokenized_test = test_dataset.map(preprocess_function, batched=True)\n",
    "\n",
    "cols_to_remove = [text_column_name, label_column_name, \"text_cleaned\", \"__index_level_0__\"]\n",
    "tokenized_train = tokenized_train.remove_columns([c for c in cols_to_remove if c in tokenized_train.column_names])\n",
    "tokenized_val = tokenized_val.remove_columns([c for c in cols_to_remove if c in tokenized_val.column_names])\n",
    "tokenized_test = tokenized_test.remove_columns([c for c in cols_to_remove if c in tokenized_test.column_names])\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=num_labels_actual)\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "accuracy_metric = evaluate.load(\"accuracy\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return accuracy_metric.compute(predictions=predictions, references=labels)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=5,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=10,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=500,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_val,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "print(\"Starting model training...\")\n",
    "trainer.train()\n",
    "print(\"Training complete.\")\n",
    "\n",
    "trainer.save_model('nfr_model_final')\n",
    "tokenizer.save_pretrained('nfr_model_final')\n",
    "print(\"Model saved to 'nfr_model_final'.\")\n",
    "\n",
    "print(\"\\n--- Model Evaluation ---\")\n",
    "class_names_ordered = le.classes_.tolist()\n",
    "\n",
    "print(\"\\nEvaluating on Training Set:\")\n",
    "train_predictions_output = trainer.predict(tokenized_train)\n",
    "train_logits = train_predictions_output.predictions\n",
    "train_predicted_numeric_labels = np.argmax(train_logits, axis=1)\n",
    "train_actual_numeric_labels = tokenized_train['label']\n",
    "print(\"Classification Report for Training Set:\")\n",
    "print(classification_report(train_actual_numeric_labels, train_predicted_numeric_labels, target_names=class_names_ordered, zero_division=0))\n",
    "\n",
    "print(\"\\nEvaluating on Test Set:\")\n",
    "test_predictions_output = trainer.predict(tokenized_test)\n",
    "test_logits = test_predictions_output.predictions\n",
    "test_predicted_numeric_labels = np.argmax(test_logits, axis=1)\n",
    "test_actual_numeric_labels = tokenized_test['label']\n",
    "print(\"Classification Report for Test Set:\")\n",
    "print(classification_report(test_actual_numeric_labels, test_predicted_numeric_labels, target_names=class_names_ordered, zero_division=0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5885c54e",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "print(\"\\n--- Custom Text Prediction Example ---\")\n",
    "\n",
    "custom_texts = [\n",
    "    \"Given the convoluted dependencies and lack of documentation, making updates felt like defusing a bomb.\",\n",
    "    \"Why do I need to click five different tabs to set a reminder? Siri could do this in one breath.\",\n",
    "    \"All inter-service communication must employ mutual TLS with certificate rotation every 24 hours.\",\n",
    "    \"UI’s got them retro 90s vibes, and not in a good way — I needed a manual to figure out what each icon meant.\"\n",
    "]\n",
    "\n",
    "custom_data_dict = {'text_cleaned': custom_texts}\n",
    "custom_hf_dataset = Dataset.from_dict(custom_data_dict)\n",
    "\n",
    "tokenized_custom_dataset = custom_hf_dataset.map(lambda examples: tokenizer(examples[\"text_cleaned\"], truncation=True, padding=True), batched=True)\n",
    "tokenized_custom_dataset = tokenized_custom_dataset.remove_columns([\"text_cleaned\"])\n",
    "\n",
    "custom_predictions_output = trainer.predict(tokenized_custom_dataset)\n",
    "custom_logits = custom_predictions_output.predictions\n",
    "custom_predicted_numeric_labels = np.argmax(custom_logits, axis=1)\n",
    "\n",
    "print(\"Predictions for custom texts:\")\n",
    "for i in range(len(custom_texts)):\n",
    "    predicted_class_name = class_names_ordered[custom_predicted_numeric_labels[i]]\n",
    "    print(f\"Text: \\\"{custom_texts[i]}\\\" -> Predicted Class: {predicted_class_name}\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
