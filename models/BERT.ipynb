{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10c3c106",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "!pip install evaluate\n",
    "import torch\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from datasets import Dataset\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, DataCollatorWithPadding, TrainingArguments, Trainer\n",
    "import evaluate\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "data_path = \"NFR_data.csv\"\n",
    "text_column_name = \"sentence\"\n",
    "label_column_name = \"class_name\"\n",
    "model_name = \"distilbert-base-uncased\"\n",
    "test_size = 0.2\n",
    "\n",
    "class Cleaner():\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def normalize_line_breaks(self, text):\n",
    "        return str(text).replace('\\r\\n', '\\n').replace('\\r', '\\n')\n",
    "\n",
    "    def remove_html_tags(self, text):\n",
    "        return BeautifulSoup(str(text), \"lxml\").text\n",
    "\n",
    "    def clean(self, text):\n",
    "        text = self.normalize_line_breaks(text)\n",
    "        text = self.remove_html_tags(text)\n",
    "        return text\n",
    "\n",
    "try:\n",
    "    df = pd.read_csv(data_path, encoding=\"latin1\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: The file {data_path} was not found. Please ensure it's in the correct path.\")\n",
    "    exit()\n",
    "except Exception as e:\n",
    "    print(f\"Error reading CSV: {e}\")\n",
    "    exit()\n",
    "\n",
    "df.dropna(subset=[text_column_name, label_column_name], inplace=True)\n",
    "df['text_cleaned'] = df[text_column_name].astype(str).apply(Cleaner().clean)\n",
    "\n",
    "le = preprocessing.LabelEncoder()\n",
    "df['label'] = le.fit_transform(df[label_column_name].astype(str).tolist())\n",
    "num_labels_actual = len(le.classes_)\n",
    "print(f\"Found {num_labels_actual} unique labels: {le.classes_}\")\n",
    "\n",
    "df_train, df_test = train_test_split(df, test_size=test_size, stratify=df['label'], random_state=42)\n",
    "\n",
    "train_dataset = Dataset.from_pandas(df_train)\n",
    "test_dataset = Dataset.from_pandas(df_test)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    return tokenizer(examples[\"text_cleaned\"], truncation=True, padding=True)\n",
    "\n",
    "tokenized_train = train_dataset.map(preprocess_function, batched=True)\n",
    "tokenized_test = test_dataset.map(preprocess_function, batched=True)\n",
    "\n",
    "tokenized_train = tokenized_train.remove_columns([text_column_name, label_column_name, \"text_cleaned\", \"__index_level_0__\"])\n",
    "tokenized_test = tokenized_test.remove_columns([text_column_name, label_column_name, \"text_cleaned\", \"__index_level_0__\"])\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=num_labels_actual)\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "accuracy_metric = evaluate.load(\"accuracy\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return accuracy_metric.compute(predictions=predictions, references=labels)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    learning_rate=5e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=10,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=500,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_test,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "print(\"Starting model training...\")\n",
    "trainer.train()\n",
    "print(\"Training complete.\")\n",
    "\n",
    "trainer.save_model('nfr_model_final')\n",
    "tokenizer.save_pretrained('nfr_model_final')\n",
    "print(\"Model saved to 'nfr_model_final'.\")\n",
    "\n",
    "print(\"\\n--- Model Evaluation ---\")\n",
    "class_names_ordered = le.classes_.tolist()\n",
    "\n",
    "print(\"\\nEvaluating on Training Set:\")\n",
    "train_predictions_output = trainer.predict(tokenized_train)\n",
    "train_logits = train_predictions_output.predictions\n",
    "train_predicted_numeric_labels = np.argmax(train_logits, axis=1)\n",
    "train_actual_numeric_labels = tokenized_train['label']\n",
    "print(\"Classification Report for Training Set:\")\n",
    "print(classification_report(train_actual_numeric_labels, train_predicted_numeric_labels, target_names=class_names_ordered, zero_division=0))\n",
    "\n",
    "print(\"\\nEvaluating on Test Set:\")\n",
    "test_predictions_output = trainer.predict(tokenized_test)\n",
    "test_logits = test_predictions_output.predictions\n",
    "test_predicted_numeric_labels = np.argmax(test_logits, axis=1)\n",
    "test_actual_numeric_labels = tokenized_test['label']\n",
    "print(\"Classification Report for Test Set:\")\n",
    "print(classification_report(test_actual_numeric_labels, test_predicted_numeric_labels, target_names=class_names_ordered, zero_division=0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcc7b192",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "print(\"\\n--- Custom Text Prediction Example ---\")\n",
    "\n",
    "custom_texts = [\n",
    "    \"Developers must be able to replace modules without affecting the rest of the system.\",\n",
    "    \"The system should include unit tests for at least 80% of the code.\",\n",
    "    \"The user interface needs to be intuitive for novice users.\",\n",
    "    \"The Proposer shall test application enhancements, fixes, and upgrades and assure the integrity of the resulting data.\"\n",
    "]\n",
    "\n",
    "custom_data_dict = {'text_cleaned': custom_texts}\n",
    "custom_hf_dataset = Dataset.from_dict(custom_data_dict)\n",
    "\n",
    "tokenized_custom_dataset = custom_hf_dataset.map(lambda examples: tokenizer(examples[\"text_cleaned\"], truncation=True, padding=True), batched=True)\n",
    "tokenized_custom_dataset = tokenized_custom_dataset.remove_columns([\"text_cleaned\"])\n",
    "\n",
    "custom_predictions_output = trainer.predict(tokenized_custom_dataset)\n",
    "custom_logits = custom_predictions_output.predictions\n",
    "custom_predicted_numeric_labels = np.argmax(custom_logits, axis=1)\n",
    "\n",
    "print(\"Predictions for custom texts:\")\n",
    "for i in range(len(custom_texts)):\n",
    "    predicted_class_name = class_names_ordered[custom_predicted_numeric_labels[i]]\n",
    "    print(f\"Text: \\\"{custom_texts[i]}\\\" -> Predicted Class: {predicted_class_name}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
