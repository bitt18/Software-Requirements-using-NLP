{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e272d5a3",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "!pip install evaluate\n",
    "import torch\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from datasets import Dataset\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, DataCollatorWithPadding, TrainingArguments, Trainer\n",
    "from transformers import EarlyStoppingCallback, logging as transformers_logging\n",
    "import evaluate\n",
    "import transformers\n",
    "\n",
    "transformers_logging.set_verbosity_error()\n",
    "\n",
    "DATA_PATH = \"NFR_five4.csv\"\n",
    "TEXT_COL = \"sentence\"\n",
    "LABEL_COL = \"class_name\"\n",
    "MODEL_NAME = \"distilbert-base-uncased\"\n",
    "TEST_SIZE = 0.2\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "class Cleaner:\n",
    "    def normalize_line_breaks(self, text):\n",
    "        return str(text).replace('\\r\\n', '\\n').replace('\\r', '\\n')\n",
    "\n",
    "    def remove_html_tags(self, text):\n",
    "        return BeautifulSoup(str(text), \"lxml\").text\n",
    "\n",
    "    def clean(self, text):\n",
    "        text = self.normalize_line_breaks(text)\n",
    "        text = self.remove_html_tags(text)\n",
    "        return text\n",
    "\n",
    "try:\n",
    "    df = pd.read_csv(DATA_PATH, encoding=\"latin1\")\n",
    "except FileNotFoundError:\n",
    "    raise SystemExit(f\"Error: The file {DATA_PATH} was not found.\")\n",
    "\n",
    "df.dropna(subset=[TEXT_COL, LABEL_COL], inplace=True)\n",
    "cleaner = Cleaner()\n",
    "df['text_cleaned'] = df[TEXT_COL].astype(str).apply(cleaner.clean)\n",
    "\n",
    "le = preprocessing.LabelEncoder()\n",
    "df['label'] = le.fit_transform(df[LABEL_COL].astype(str).tolist())\n",
    "class_names = le.classes_\n",
    "num_labels = len(class_names)\n",
    "print(f\"Found {num_labels} unique labels: {class_names}\")\n",
    "\n",
    "labels = df['label'].values\n",
    "class_weights_np = compute_class_weight(class_weight='balanced', classes=np.arange(num_labels), y=labels)\n",
    "class_weights = torch.tensor(class_weights_np, dtype=torch.float)\n",
    "counts = np.bincount(labels, minlength=num_labels)\n",
    "prior = counts / counts.sum()\n",
    "class_bias = torch.log(torch.tensor(prior, dtype=torch.float))\n",
    "\n",
    "df_train, df_test = train_test_split(df, test_size=TEST_SIZE, stratify=df['label'], random_state=RANDOM_STATE)\n",
    "\n",
    "d_train = Dataset.from_pandas(df_train)\n",
    "d_test = Dataset.from_pandas(df_test)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "def preprocess_fn(examples):\n",
    "    return tokenizer(examples['text_cleaned'], truncation=True, padding=True)\n",
    "\n",
    "tok_train = d_train.map(preprocess_fn, batched=True)\n",
    "tok_test  = d_test.map(preprocess_fn, batched=True)\n",
    "\n",
    "drop_cols = [TEXT_COL, LABEL_COL, 'text_cleaned', '__index_level_0__']\n",
    "tok_train = tok_train.remove_columns([c for c in drop_cols if c in tok_train.column_names])\n",
    "tok_test  = tok_test.remove_columns([c for c in drop_cols if c in tok_test.column_names])\n",
    "\n",
    "class CustomModel(torch.nn.Module):\n",
    "    def __init__(self, model_name, num_labels, class_weights, class_bias):\n",
    "        super().__init__()\n",
    "        self.model = AutoModelForSequenceClassification.from_pretrained(\n",
    "            model_name,\n",
    "            num_labels=num_labels\n",
    "        )\n",
    "        self.model.classifier.bias.data = class_bias.to(self.model.device)\n",
    "        self.class_weights = class_weights\n",
    "\n",
    "    def forward(self, input_ids=None, attention_mask=None, labels=None, **kwargs):\n",
    "        kwargs.pop('num_items_in_batch', None)\n",
    "        outputs = self.model(input_ids=input_ids, attention_mask=attention_mask, **kwargs)\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss_fct = torch.nn.CrossEntropyLoss(weight=self.class_weights.to(self.model.device))\n",
    "            loss = loss_fct(outputs.logits, labels)\n",
    "        return transformers.modeling_outputs.SequenceClassifierOutput(\n",
    "            loss=loss,\n",
    "            logits=outputs.logits,\n",
    "            hidden_states=outputs.hidden_states,\n",
    "            attentions=outputs.attentions\n",
    "        )\n",
    "\n",
    "model = CustomModel(MODEL_NAME, num_labels, class_weights, class_bias)\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "metric = evaluate.load('accuracy')\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    preds = np.argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions=preds, references=labels)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    learning_rate=5e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=10,\n",
    "    weight_decay=0.05,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=10,\n",
    "    eval_strategy='steps',\n",
    "    save_strategy='steps',\n",
    "    save_steps=50,\n",
    "    eval_steps=50,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model='accuracy',\n",
    "    greater_is_better=True\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tok_train,\n",
    "    eval_dataset=tok_test,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=2)]\n",
    ")\n",
    "\n",
    "print(\"Starting training...\")\n",
    "trainer.train()\n",
    "print(\"Training complete.\")\n",
    "\n",
    "trainer.save_model('nfr_model_final')\n",
    "tokenizer.save_pretrained('nfr_model_final')\n",
    "print(\"Model saved to 'nfr_model_final'.\")\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "def eval_and_report(dataset, name):\n",
    "    print(f\"\\nEvaluating on {name} set:\")\n",
    "    output = trainer.predict(dataset)\n",
    "    preds = np.argmax(output.predictions, axis=1)\n",
    "    labels = dataset['label']\n",
    "    print(classification_report(labels, preds, target_names=class_names, zero_division=0))\n",
    "\n",
    "print(\"\\n--- Model Evaluation ---\")\n",
    "eval_and_report(tok_train, 'Training')\n",
    "eval_and_report(tok_test, 'Test')\n",
    "\n",
    "print(\"\\n--- Custom Text Prediction Example ---\")\n",
    "custom_texts = [\n",
    "    \"No unauthorized access should be permitted to sensitive data.\",\n",
    "    \"The system must respond to user queries within 3 seconds.\",\n",
    "    \"The user interface needs to be intuitive for novice users.\",\n",
    "    \"The Proposer shall test application enhancements, fixes, and upgrades and assure the integrity of the resulting data.\"\n",
    "]\n",
    "custom_ds = Dataset.from_dict({'text_cleaned': custom_texts})\n",
    "custom_tok = custom_ds.map(lambda ex: tokenizer(ex['text_cleaned'], truncation=True, padding=True), batched=True)\n",
    "custom_tok = custom_tok.remove_columns(['text_cleaned'])\n",
    "\n",
    "pred_out = trainer.predict(custom_tok)\n",
    "preds = np.argmax(pred_out.predictions, axis=1)\n",
    "\n",
    "print(\"Predictions for custom texts:\")\n",
    "for txt, idx in zip(custom_texts, preds):\n",
    "    print(f\"Text: '{txt}' -> Predicted Class: {class_names[idx]}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2be76a5c",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "print(\"\\n--- Custom Text Prediction Example ---\")\n",
    "\n",
    "custom_texts = [\n",
    "    \"No unauthorized access should be permitted to sensitive data.\",\n",
    "    \"The system must respond to user queries within 3 seconds.\",\n",
    "    \"All aspects of the website shall be accessed by a web browser over the Internet.\",\n",
    "    \"The Proposer shall test application enhancements, fixes, and upgrades and assure the integrity of the resulting data.\",\n",
    "    \"All details about cardmembers must be retrieved from the Cardmember Information Database.\"\n",
    "]\n",
    "\n",
    "custom_data_dict = {'text_cleaned': custom_texts}\n",
    "custom_hf_dataset = Dataset.from_dict(custom_data_dict)\n",
    "\n",
    "tokenized_custom_dataset = custom_hf_dataset.map(lambda examples: tokenizer(examples[\"text_cleaned\"], truncation=True, padding=True), batched=True)\n",
    "tokenized_custom_dataset = tokenized_custom_dataset.remove_columns([\"text_cleaned\"])\n",
    "\n",
    "custom_predictions_output = trainer.predict(tokenized_custom_dataset)\n",
    "custom_logits = custom_predictions_output.predictions\n",
    "custom_predicted_numeric_labels = np.argmax(custom_logits, axis=1)\n",
    "\n",
    "print(\"Predictions for custom texts:\")\n",
    "for i in range(len(custom_texts)):\n",
    "    predicted_class_name = class_names_ordered[custom_predicted_numeric_labels[i]]\n",
    "    print(f\"Text: \\\"{custom_texts[i]}\\\" -> Predicted Class: {predicted_class_name}\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
